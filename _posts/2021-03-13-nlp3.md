---
title: "[Paper Review] Supervised Learning of Universal Sentence Representations from Natural Language Inference Data"
date: 2021-03-13 10:000 -0400
author : 정여진
categories :
  - paper-review
  - NLP
tags :
  - NLP
  - deep-learning
---

Today, we will review a paper on sentence encoder, published in 2018. 

# Introduction / Related Works

Many researches on word embeddings, from Word2vec, GloVe, to FastText. have been successful. However, researches on embedding for large corpus of words such as sentences, have been lacking in comparison. The central issue of embedding sentences is capturing relationships among multiple words and phrases in a single vector. The most recent works (up to 2018) approached this issue with **unsupervised learning**. Examples are :

- SkipThought (Kiros et al,. 2015) : abstracts skip-gram model to sentence level / RNN-RNN encoder decoder model with GRU activation
- FastSent (Hill et al., 2016) : additive log-linear sentence model

The reason for this preference is to prevent model from specializing too much on biases on task and keep overall information or semantics. Unsupervised task is more advantageous. On the other hand, this paper explores whether universal representations of sentences can be trained in **supervised** manner.

### Motivation
Motivaiton of supervised approach comes from recent works in computer vision. Transfer learning using supervised features have been successful in _computer vision_. Examples are face recognition (Taigman et al.,2014) and visual question answering(Antol et al.,2015) where image features trained on ImageNet (which is all labelled) and word embeddings trained on large unsupervised corpora are combined.

# Approach

### Data / Task

![2021-03-13-1](/assets/2021-03-13-supervised1.png)

Model is trained on **Stanford Natural Language Inference (SNLI)** dataset which consists of 570k human generated English sentence pairs with three categories - _entailment, contradiction, and neutral_.

### Method
**"Universal representations of sentences are learned via separate sentence encoding-based models on SNLI task."**

![2021-03-13-1](/assets/2021-03-13-supervised2.png)

Firstly, to encode sentences separately into fixed size representations, seven different architectures are compared. 

- Long Short-Term Memory (LSTM)
- Gated Recurrent Units (GRU)
- Bi-directional GRU with concat of last hidden states (BiGRU)
- Bi-directional LSTM with mean pooling (BiLSTM-Mean)
- **Bi-directional LSTM with max pooling (BiLSTM-Max) <<BEST!**
- Inner-attention
- Hierarchical ConvNet (HConvNet)


Next, once the sentence representations for premise u and hypothesis v are generated, three matching methods are applied to extract relationships : concat / element=wise product / absolute element-wise difference

Finally, use MLP with 1 hidden layer of 512 hidden units as classifier.

### BiLSTM & Inner Attentive Network

(Refer to another post :) )


# Evaluation / Conclusion
Each encoder is tested on several transfer tasks including _classification tasks_ (binary and multiclass, entailment and semantic relatedness, semantic textual similarity etc. ) and _natural language inference (NLI)_ and _semantic textual similarity (STS) tasks_. The proposed encoders were also tested with other methods such as
> word2vec, GloVe, DBOW, TF-IDF, FastSent, SkipThought, 

![2021-03-13-1](/assets/2021-03-13-supervised3.png)

When this paper came out, the most well performing sentence encoder was **SkipThought-LN**. While this model takes over a month to train, BiLSTM-MAX succeeded in better performance for _MR, CR, and MPQA_ with much less train data. 
